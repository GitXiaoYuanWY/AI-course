# 文生成图完整运行逻辑的秘密公式

# 板块一：课程概述

## 一、课程回顾

1. 理解 Stable Diffusion 的核心基础概念与作用
2. 明确 ComfyUI 的学习重点与工作逻辑
3. 熟悉主流模型下载网站及其适用场景
4. 认识常见模型类型与对应的风格表现
5. 掌握模型下载与导入 ComfyUI 的完整流程

## 二、学习目标

1. 了解了文生图（Text-to-Image）的发展脉络与核心理念
2. 掌握了文生图的核心运行逻辑
3. 认识了文本编码的重要性与基本原理
4. 学习了提示词（Prompt）的结构、写法与优化技巧

## 三、课程目录

1. 文生图概述：从文本到图像的核心思想
2. 文生图核心运行逻辑
3. 文本编码
4. 提示词（Prompt）的写作方法

# 板块二：课程章节

## 一、文生图概述：从文本到图像的核心思想

### （一）文生图的定义与发展背景

- 定义：文生图，英文 Text-to-Image，是指用户只需要输入一段文字描述，系统就能自动生成与文字语义对应的图像。这个能力并不是凭空想象出来的，而来自模型对大量图像与文字之间对应关系的学习。
- 发展背景：早期图像生成主要依靠 GAN 模型，虽然可以生成逼真的图像，但缺乏对语义的精确控制。后来出现 VQGAN+CLIP 的方法，能够在一定程度上根据文字生成画面，但速度慢、不稳定。
- 真正让文生图进入大众视野的是扩散模型（Diffusion Model），尤其是 Stable Diffusion 的开源，使普通电脑也能本地生成高质量图像。

### （二）文生图应用场景

- 文生图并不是玩具，它正在改变多个行业：
    - **原创设计**：帮助设计师进行创意发散、草图生成。
    - **游戏与影视前期制作**：快速产出概念图、角色设计、环境构想。
    - **广告媒体行业**：自动化生成海报、KV、场景素材。
    - **个人创作**：普通用户也能创作原本需要多年美术训练才能完成的图像。
- 文生图的价值不在于取代创意，而在于加速与扩展创意的可能性。

## 二、文生图核心运行逻辑

### （一）整体流程框架

- 文生图的过程可以概括为一个逻辑链条：
    - **Prompt（文本描述）
    → 文本编码
    → 随机噪声
    → U-Net 逐步去噪
    → Sampler 决定降噪方式
    → VAE 解码
    → 最终图像**
- 也就是说，模型不是凭空画图，而是从一张完全随机的“噪声图”开始，通过不断消除无关噪声，让图像逐渐呈现出来。

### （二）潜空间与像素空间的关系

- Stable Diffusion 的一个关键突破，是：它不是直接在 512×512 的像素图上生成，而是在更小的“潜空间”上工作。
    - 什么是**“潜空间”：**指模型在学习过程中将**复杂的高维输入数据（如图像、文本、声音）通过编码器（Encoder）压缩、映射到一个更低维的、连续的、抽象的向量空间**。
- 潜空间的尺寸通常是：1×4×64×64
- 它相当于对图像进行了一次高效压缩，将图片表达为结构+语义的组合形式。
- 在潜空间生成 → 计算量更小 → 显存更低 → 成图速度更快。

## 三、文本编码

### （一）文本编码的意义

- 在文生图模型中，文本是生成的起点。模型必须先“理解”文字，才能据此生成符合语义的图像。这种“理解”并非语言学层面的理解，而是将语言转化为可计算的数学向量表示，即文本嵌入（Text Embedding）。这一过程由 文本编码器（Text Encoder） 完成，它的任务是把自然语言映射到潜空间中。
- 简而言之：文本编码 = 把文字的语义 → 转换成数字化的语义坐标。

### （二）文本到向量：从词汇到语义

- 分词（Tokenization）
    - 模型首先将一句话分解成一个个最小可识别的语言单位——**token**。
        - 例如：“一只蓝色的猫坐在窗台上”
            
            → `[一只, 蓝色, 的, 猫, 坐在, 窗台, 上]`
            
        - 对英文模型，如 CLIP 或 BERT，则会使用 **Byte-Pair Encoding (BPE)** 或 **SentencePiece**，保证语言灵活性与可扩展性。
- 词嵌入（Word Embedding）
    - 每个 token 会被映射成一个高维向量。例如，一个 768 维的向量可以表示“猫”的语义特征。这些向量不仅包含字面意义，还会学习到上下文关系：
        - “猫”在“蓝色的猫”与“猫咖啡馆”中的向量是不同的。
        - 模型通过上下文捕捉语义细微差异。
- 上下文理解（Contextual Encoding）
    - 文本编码器（通常为 Transformer）会基于上下文重新组合这些向量，使模型理解整句含义，而非孤立单词。
        - 例如，模型能区分“蓝猫在窗台上”和“窗台上的蓝猫”虽然词相同但语义略有差异。

## 四、提示词（Prompt）的写作方法

### （一）提示词的基本结构

- 一个完整的提示词可分为四层：
    1. **主体元素（Subject）**
        - 例：“a cyberpunk city at night”
        - 决定图像的核心对象。
    2. **风格修饰（Style Modifiers）**
        - 例：“in the style of Studio Ghibli, watercolor painting”
        - 控制图像的艺术风格。
    3. **细节控制（Detail Modifiers）**
        - 例：“high detail, dramatic lighting, 8K resolution”
        - 影响画面质感与复杂度。
    4. **情绪与氛围（Mood & Composition）**
        - 例：“melancholic, cinematic composition, foggy atmosphere”
        - 提供整体情绪基调。

### （二）提示词的逻辑顺序

文生图模型对提示词顺序非常敏感。推荐结构：

```
[主体] + [动作/状态] + [环境] + [风格] + [光线] + [细节修饰]
```

例如：

> A lone samurai standing in a snowy forest, cinematic lighting, concept art, ultra detailed.
> 

### （三）权重与组合技巧

- 权重控制：使用括号强化重点，如 `(cat:1.5)`；
- 并列构图：用 “AND” 或 “with” 表示并存；
- 对比提示：用“without”、“no”引导负面提示；
- 多模型融合：通过 ControlNet、Lora 等技术加载不同模型特征。

### （四）负面提示词（Negative Prompts）

负面提示用于约束不想要的元素：

- 常用示例：
    
    ```
    blurry, deformed, watermark, text, low contrast, bad anatomy, extra limbs
    ```
    
- 在实际生成中，负面提示的重要性不亚于正面提示。
    
    它相当于模型的“纠错机制”，确保图像干净、自然、构图合理。
    

## 五、文生图在 ComfyUI 中的实际应用

### （一）搭建文生图基础工作流

- 参考：
    
    ![image.png](image.png)
    
- 应用到的节点：
    - Checkpoint加载器(简易)
    - CLIP文本编码
    - K采样器
    - 空Latent图像
    - VAE解码
    - 保存图像

### （二）运行操作

- 第一步：如图链接好各个节点
    
    ![image.png](image.png)
    
- 第二步：输入正反提示词
    
    ![image.png](image%201.png)
    
- 第三步：注意步数和cfg值的调试
    
    ![image.png](image%202.png)
    
- 第四步：点击生成即可
    
    ![image.png](image%203.png)
    
- 效果图参考
    
    ![image.png](image%204.png)
    

# 板块三：课后作业

- 自己尝试写一份用于生图的正提示词和负提示词